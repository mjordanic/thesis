\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Appendix: Pattern recognition}

\begin{appendix}
\chapter{Pattern recognition}
Pattern recognition is a classification technique and the principle by which it is performed is learned independently from the data, i.e., training set. There are two main types of pattern recognition: supervised and unsupervised. Supervised pattern recognition implies that the classes of the training set are known and are used to obtain the model. New inputs are identified as one of the predetermined classes. On the other hand, unsupervised pattern recognition is used when no labels are available and samples are assigned to unknown classes. This technique is more appropriate for the clustering problem because the classes are determined automatically by the system, whereas supervised approach is more appropriate for the classification because classes are defined by the system designer, and, therefore, it is usually used in task identification based on EMG.

In statistical pattern recognition, each sample is composed of $m$ measures that form the pattern, i.e., features $(x_0, x_1, \dots, x_{m-1})$. The objective of the algorithm is to obtain a decision rule, i.e., the decision boundary which separates well samples of different classes. There are many \emph{state-of-the-art} classifiers that use various principles to construct these boundaries. However, many researchers agree that the fidelity of the classification in EMG applications depends mostly on selection of features \citep{Hakonen2015}. In other words, with appropriate selection of features, all classifiers will give similar classification result. A short introduction is provided on the two methods used in the thesis: Linear Discriminant Analysis and Support Vector Machine. Although the SVM is superior in classification performance, the LDA is commonly used in myocontrol applications because of its simplicity and performance in real-time. However, with the increasing computational power of new computer generations, SVM could become more common in these applications. 


\section{Linear Discriminant Analysis}

\begin{myquote}
\begin{flushright}
\textit{All models are wrong; some models are useful.} \\George E. P. Box
\end{flushright}
\end{myquote}

Linear Discriminant Analysis is a computationally simple and efficient classifier with linear decision boundary and it is based on the Bayesian equation. In a classical problem with $n$ samples in training set, which consist of $m$ features, the dataset of available samples is a matrix of dimension $[n \times m]$, whereas the label that describes the belonging of each sample to one of the classes is $y$, where $y \in (0,1,2,...,K-1)$.

According to Bayesian equation, the probability that a sample $\mathbf{x}_0$ belongs to a class $k$ is equivalent to the: 
\begin{equation} 
P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0) = \frac{P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=k) \,\,P(y=k)} {P(\mathbf{x}=\mathbf{x}_0)}
\end{equation}
\noindent
, where k represents the class. Term $P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=k)$ is called the \emph{class-conditional} probability and describes the probability that the sample with exact features $\mathbf{x}_0$ is encountered within the group of samples belonging to the class $k$. Term $P(y=k)$ is called the \emph{a priori} probability and describes the probability that the sample belonging to the class $k$ is found within the group of all samples, regardless of the features. Finally, the term $P(\mathbf{x}=\mathbf{x}_0)$ is called the \emph{marginal} probability and describes the probability of finding the sample with exact set of features in the dataset, regardless of the class. Marginal probability can be written as a sum of class-conditional probabilities multiplied by the a priori probabilities for each class:
\begin{equation}
\begin{split}
P(\mathbf{x}=\mathbf{x}_0) = P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=1) \,\,P(y=1) + \\
P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=2) \,\,P(y=2) + \dots + \\
P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=K) \,\,P(y=K)
\end{split}
\end{equation}

Following the Bayesian theory, the hypothesis, i.e., the predicted class of a sample $\mathbf{x}_0$ is chosen as the class which has the highest probability $P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0)$:
\begin{equation} 
h(\mathbf{x}_0) = \argmax_k P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0)
\end{equation}

Statistically speaking, this is the best possible classifier. The problem arises in the implementation. The exact probability density functions are unknown and have to be estimated from the available data, which is the source of error.
Estimated version of the stated probabilities will be marked with a different symbols to stress out the fact they are just an estimates:
\begin{equation} 
p_k(\mathbf{x}) \coloneqq P(y=k \,\, | \, \mathbf{x})
\end{equation}
\begin{equation} 
g_k(\mathbf{x}) \coloneqq P(\mathbf{x}  \,\, | \, y=k)
\end{equation}
\begin{equation} 
\pi_k \coloneqq P(y=k)
\end{equation}


Linear Discriminant Analysis estimates marginal probability term ($\pi_k$) as a ratio of number of samples belonging to class $k$ and the total  number of samples, whereas the class-conditional probability term in the Bayesian equation is estimated as a multivariate Gaussian function:
\begin{equation} 
g_k(\mathbf{x}) = \frac{1}{(2 \pi)^{\nicefrac{m}{2}}\, |\Sigma_k|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_k)^T  \Sigma_k^{-1} (\mathbf{x}-\mu_k)} 
\end{equation}
\noindent
, where $m$ is the dimensionality of the feature space, i.e., number of features representing each sample. Function $g_k$ is estimated class-conditional probability of class $k$, and $\mu_k$ and $\Sigma_k$ are the mean and co-variance matrix for class $k$, respectively, and they are estimated from the available data as:
\begin{equation}
\left. \mu_k = \frac{1}{n_k} \sum_{i}{\mathbf{x}_i} \right\vert_{\forall \mathbf{x} \in k}
\end{equation}
\begin{equation}
\left. \Sigma_k = \frac{1}{n_k-K} \sum_{i}{\Big( \mathbf{x}_i - \mu_k \Big) \Big( \mathbf{x}_i - \mu_k \Big)^T} \right\vert_{\forall \mathbf{x} \in k}
\end{equation}
\noindent
, where $n_k$ represents the number of samples belonging to a class $k$.
To simplify the model, LDA assumes that the co-variance matrices $\Sigma_k$ are the same for all classes:
\begin{equation} 
\Sigma_0 = \Sigma_1 = \dots = \Sigma_{K-1} = \Sigma
\end{equation}
\noindent
and they are usually calculated using the weighted average:
\begin{equation} 
\Sigma = \frac{\sum_{k=1}^K {n_k\Sigma_k}}{\sum_{k=1}^K{n_k}}
\end{equation}

\noindent
The consequence of this assumption is the linearity of the decision boundary. Without this assumption the same calculus would lead to quadratic discriminant analysis, which has non-linear boundary.

In a two class example ($y \in \{0,1\}$), all samples on the decision boundary will have the same probability of belonging to class 0 or 1:
\begin{equation} 
D.B. = \Big\{\mathbf{x}\,\, \Big| \,\,P\big(y=0 \, \big| \, \mathbf{x}=\mathbf{x}_0\big) = P\big(y=1 \, \big| \, \mathbf{x}=\mathbf{x}_0\big) \Big\}
\end{equation}

Following this idea, the decision boundary can be estimated by solving the equation:
\begin{equation} 
\frac{g_0(\mathbf{x}) \, \pi_0} {\cancel{\sum_{k=1}^K{g_k \, \pi_k}}} = \frac{g_1(\mathbf{x}) \, \pi_1} {\cancel{\sum_{k=1}^K{g_k \, \pi_k}}}
\end{equation}
\begin{equation} 
%\begin{split}
\frac{1}{(2 \pi)^{\nicefrac{m}{2}}\, |\Sigma_0|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_0)^T  \Sigma_0^{-1} (\mathbf{x}-\mu_0)}  \,\,\,\pi_0 = \\
\frac{1}{(2 \pi)^{\nicefrac{m}{2}}\, |\Sigma_1|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_1)^T  \Sigma_1^{-1} (\mathbf{x}-\mu_1)}  \,\,\,\pi_1
%\end{split}
\end{equation}

%,where $d$ is the dimensionality of the feature space, i.e., number of features representing each sample.
If making the assumption on the equal co-variance matrices for both classes $(\Sigma_0 = \Sigma_1  = \Sigma)$, and taking the logarithm, the equation takes the form: 
\begin{equation} 
%\begin{split}
-\frac{1}{2}  \Big(\mathbf{x}-\mu_0\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_0\Big) +  \log{\Big(\pi_0\Big)}= \\
-\frac{1}{2}  \Big(\mathbf{x}-\mu_1\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_1\Big) +  \log{\Big(\pi_1\Big)}
%\end{split}
\end{equation}
, which can be written in the form of the linear function $x^T\beta + \alpha = 0$ as:
\begin{equation} 
\begin{split}
\mathbf{x}^T\Big(\Sigma^{-1} \mu_0 - \Sigma^{-1} \mu_1\Big) + \frac{1}{2} \Big(\mu_1^T \Sigma^{-1}\mu_1 - \mu_0^T \Sigma^{-1}\mu_0\Big)
+ \log\Big( \frac{\pi_0}{\pi_1}  \Big) = 0
\end{split}
\end{equation}

This equation represents the decision boundary between two classes, i.e., all samples lying on this line will have equal probability of belonging to class 0 and class 1. It should be noted that the slope of the line depends only on the class means and co-variance matrix, whereas a priori probabilities (which are the result of number of samples belonging to class 0 or 1) have effect only on the $y$-intercept term, i.e., the offset of the function. This is an interesting point that demands caution. If groups are unbalanced, that is, number of samples of one group is higher than in the other group, $y$-intercept of the decision boundary will be affected and the classifier will be biased by this disproportion. If groups are unbalanced because of the incomplete or missing data, whereas in reality they are balanced, this can have a negative effect.

When considering multiclass classification problem, probability of a sample belonging to each class is firstly estimated by the equation:
\begin{equation}
p_k = -\frac{1}{2} \log \big\vert \Sigma \big\vert - \frac{1}{2}  \Big(\mathbf{x}-\mu_k\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_k\Big) +  \log{\Big(\pi_k\Big)}
\end{equation}
\noindent and then the class is estimated as the one with the highest probability as:
\begin{equation} 
h(\mathbf{x}) = \argmax_k p_k(\mathbf{x})
\end{equation}


\section{Support Vector Machine}

\begin{myquote}
\begin{flushright}
\textit{Try to solve the problem directly and never solve a more general problem as an intermediate step.} \\Vladimir Vapnik
\end{flushright}
\end{myquote}


Support vector machine is nowadays known as a very powerful classifier with a lot of different applications. The big advantage over LDA is the fact that it is a \emph{non-parametric} classifier. The model is not obtained using assumptions of the form of the class density function and estimation of it's parameters, which is inevitably erroneous. Instead, SVM forms the decision boundary using the samples (not their density estimates) by maximizing the distance between samples and the boundary.
This was the idea Vladimir Vapnik, the inventor of this method stood for. It is better to try to solve the problem directly and simply, without many intermediate steps that can be complicated and inaccurate.

%This classification method essentially has a very simple principle, but using several mathematical tricks, it became a very powerful tool. 

In pattern recognition, the decision rule ($h$) is usually obtained by multiplying the sample ($\mathbf{x}$) by predefined weights ($\Theta$):
\begin{equation} 
\Theta^T \mathbf{x} + \Theta_0
\end{equation}
\noindent , where $\Theta_0$ is a constant. If samples $\mathbf{x}_0$ and $\mathbf{x}_1$ lay on the decision boundary, following statements are true:
\begin{equation} 
\Theta^T \mathbf{x}_0 + \Theta_0 = \Theta^T \mathbf{x}_1 + \Theta_0
\end{equation}
\begin{equation} 
\Theta^T (\mathbf{x}_0 - \mathbf{x}_1) = 0
\end{equation}

\noindent This result implies that $\Theta$ is perpendicular to the boundary:
\begin{equation} 
\Theta \perp \left(\mathbf{x}_0 - \mathbf{x}_1\right)
\end{equation}


The goal of the SVM is to find the decision boundary between two classes so that the distance between the samples and the decision boundary, i.e., the margin is maximized. The distance ($d$) from a sample to the decision boundary can be defined as the distance between the sample $\mathbf{x}$ and any point lying on the boundary, $\mathbf{x}_0$, projected onto the vector $\Theta$.
\begin{equation} 
d = \frac{\Theta^T \big(\mathbf{x} - \mathbf{x}_0\big)}{\big\vert \Theta \big\vert}
\end{equation}

\noindent Term $\vert \Theta \vert$ is introduced to normalize the vector $\Theta$. Without the normalization the distance would depend on the norm of $\Theta$.

Since $\mathbf{x}_0$ is on the decision boundary, the expression $\Theta^T \mathbf{x}_0 + \Theta_0 = 0$ is valid, and, therefore, the expression for the distance can be written as:
\begin{equation} 
d = \frac{\Theta^T \mathbf{x} + \Theta_0}{\big\vert \Theta \big\vert}
\end{equation}

Margin ($M$) can be defined as the distance from the boundary to the closest sample:
\begin{equation} 
M = \min_i d_i
\end{equation}

Depending on which side of the boundary the sample is located, the distance can be positive or negative. In order to keep it strictly positive, term $y$ is introduced, where $y \in \{-1,1\}$:
\begin{equation} 
M = \min_i \big\{y_id_i\big\}
\end{equation}
\begin{equation} 
M = \min_i \left\{ \frac{y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right)}{\left\vert \Theta \right\vert} \right\}
\end{equation}

The objective is to maximize the margin $M$. Since $\Theta$ can be rescaled, a certain $\Theta$ exists so that $y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) = 1$, which implies
\begin{equation} 
\exists \Theta, \,\, y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) = 1 \,\,\,\, \Rightarrow \,\,\,\, M = \min_i \left\{ \frac{1}{\left\vert \Theta \right\vert} \right\}
\end{equation}

\noindent Therefore, to maximize the margin, a separating hyperplane should be found such that a norm of vector orthogonal to the hyperplane ($\Theta$) is minimal. 

For every point not on the boundary the following term is valid:
\begin{equation} 
y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) > 0 
\end{equation}
Value $C$ can be selected such that:
\begin{equation} 
y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) > C
\end{equation}
\begin{equation} 
y_i \left(\frac{\Theta^T \mathbf{x}_i}{C} + \frac{\Theta_0}{C}\right) > 1
\end{equation}

Since $\Theta$ and $\Theta_0$ can be rescaled, it can be written:
\begin{equation} 
\Theta := \frac{\Theta}{C}, \,\,\,\,\,\, \Theta_0 := \frac{\Theta_0}{C}
\end{equation}
%\begin{equation} 
%\Theta_0 := \frac{\Theta_0}{C}
%\end{equation}
, and, therefore:
\begin{equation}  \label{eq:SVM-constraint}
 y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) > 1
\end{equation}

Finally the optimization problem states:
\begin{equation} 
\min \frac{1}{2} \vert \Theta \vert ^2, \,\,\,\, s.t. \,\,\, y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) > 1.
\end{equation}

\noindent $L_2$ norm is preferred because it has continuous derivative, whereas constant $\nicefrac{1}{2}$ is introduced for the mathematical convenience. The optimization is solved using Lagrangian method as:
\begin{equation} \label{eq:SVM-opt}
L(\Theta, \Theta_0, \alpha_i) = \frac{1}{2} \vert \Theta \vert ^2 - \sum_{i=1}^n \alpha_i \left[y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) - 1\right]
\end{equation}
\begin{equation} \label{eq:SVM-primal}
\frac{\partial L}{\partial \Theta} = \Theta - \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i = 0 \,\,\,\, \Rightarrow \,\,\,\, \Theta = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i
\end{equation}
\begin{equation} \label{eq:SVM-dual}
\frac{\partial L}{\partial \Theta_0} = \sum_{i=1}^n \alpha_i y_i = 0
\end{equation}

By rewriting the problem in \ref{eq:SVM-opt} in terms of dual variable $\alpha$, the following expression can be obtained:
\begin{equation} \label{eq:SVM-rewritten}
L(\alpha) = \sum_i \alpha_i - \frac{1}{2} \sum_j \sum_i \alpha_j \alpha_i y_j y_i \mathbf{x}_i^T \mathbf{x}_j
\end{equation}

Since this function depends only on dual variable $\alpha$, the solution can be obtained by maximization:
\begin{equation} 
%\max \sum_i \alpha_i - \frac{1}{2} \sum_j \sum_i \alpha_j \alpha_i y_j y_i \mathbf{x}_i^T \mathbf{x}_j, \,\,\,\, s.t. \,\,\, \alpha_i \geq 0, \sum_i \alpha_i y_i = 0
%\max L(\alpha) \,\,\,\,\, s.t. \,\,\, \alpha_i \geq 0,   \sum_i \alpha_i y_i = 0
\max L(\alpha) \,\,\,\,\, s.t. \,\,\, \left\{\begin{array}{lr} \alpha_i \geq 0 \\
\sum_{i} \alpha_i y_i = 0  \end{array}\right.
\end{equation}

In this optimization problem, the objective has the form of quadratic function, whereas constraints are linear. This problem is typically solved using quadratic programming. Since it is a convex problem, the solution will always be global maximum. Once the dual variable $\alpha$ is found, the primal variable $\Theta$ can be calculated using the equation \ref{eq:SVM-primal}.

In the optimization, Karush-Kuhn-Tucker conditions need to be satisfied \citep{Boyd2004}. One of this condition is \emph{complementary slackness}, stating that in the optimal point (the solution of the problem), the product of dual variable and the constraint must be zero:
\begin{equation} 
\alpha_i \left[ y_i \left( \Theta^T\mathbf{x}_i + \Theta_0  \right) -1 \right] = 0
\end{equation}

This condition explains well the principle of SVM. Since the dual variable must be greater or equal to zero ($\alpha \geq 0$), there are two possibilities:
\begin{enumerate}
\item
If $\alpha$ is greater than zero, $\left[ y_i \left( \Theta^T\mathbf{x}_i + \Theta_0  \right) -1 \right]$ must equal one:
\begin{equation} 
\alpha_i > 0 \,\,\,\,\, \Rightarrow \,\,\,  y_i \left( \Theta^T\mathbf{x}_i + \Theta_0  \right) = 1
\end{equation}
\item
If $\left[ y_i \left( \Theta^T\mathbf{x}_i + \Theta_0  \right) -1 \right]$ is greater than zero, $\alpha$ must be zero:
\begin{equation} 
y_i \left( \Theta^T\mathbf{x}_i + \Theta_0  \right) > 1 \,\,\,\,\, \Rightarrow \,\,\,  \alpha = 0
\end{equation}
\end{enumerate}

Since for all samples lying on the margin, the statement  
\begin{equation}
y_i \left( \Theta^T\mathbf{x}_i + \Theta_0  \right) = 1
\end{equation}
\noindent holds, $\alpha$ will be greater than zero only for the samples lying on the decision hyperplane, whereas for the samples further away from the hyperplane, $\alpha$ will be zero. Given the fact that $\Theta$ depends on linear combination of samples weighted by $\alpha$ (eq. \ref{eq:SVM-primal}), only the samples lying on the boundary will have effect in the calculation of $\Theta$ (where $\alpha > 0$), and they are called \emph{support vectors}. The inconveniency of this approach is the fact that the data need to be linearly separable, i.e., there should not be any data on the other side of the margin, which is rarely the case in practice. For this reason it is called the \emph{hard margin SVM}. Margin has distance one from the boundary and all points have to be distanced more or equal (constraint in eq. \ref{eq:SVM-constraint}). To relax this constrain, variable $\beta_i$ is introduced for every sample $\mathbf{x}_i$, such that $\beta_i \geq 0$:s
\begin{equation} 
 y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) \geq 1 - \beta_i
\end{equation}

For points lying on the other side of the margin, $\beta$ will be positive, whereas for the points on the margin or on the correct side of it, it will be zero. This is the ground assumption for \emph{soft margin SVM}. The new optimization problem states:
 \begin{equation} 
\max \frac{1}{2} \vert \Theta \vert ^2 + \gamma \sum_{i=1}^n \beta_i \,\,\,\,\, s.t. \,\,\, \left\{\begin{array}{lr} y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) \geq 1 - \beta_i \\
\beta_i \geq 0  \end{array}\right.
\end{equation}
The term $\gamma \sum_{i=1}^n \beta_i$ is introduced to minimize this effect, whereas $\gamma$ is the constant of penalization. The procedure of solving the problem is the same as in hard margin SVM, using the Lagrangian method:
\begin{equation} 
L(\Theta, \Theta_0, \beta_i, \alpha_i, \lambda_i) = \frac{1}{2} \vert \Theta \vert ^2 + \gamma \sum_{i=1}^n \beta_i - \sum_{i=1}^n \alpha_i \left[y_i \left(\Theta^T \mathbf{x}_i + \Theta_0\right) - 1 + \gamma \beta_i \right] - \sum_{i=1}^n \lambda_i \beta_i
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \Theta} = \Theta - \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i = 0 \,\,\,\, \Rightarrow \,\,\,\, \Theta = \sum_{i=1}^n \alpha_i y_i \mathbf{x}_i
\end{equation}
\begin{equation}
\frac{\partial L}{\partial \Theta_0} = \sum_{i=1}^n \alpha_i y_i = 0
\end{equation}
\begin{equation} \label{dual_variable_beta}
\frac{\partial L}{\partial \beta_i} = \gamma - \alpha_i - \lambda_i = 0
\end{equation}

By rewriting the optimization problem in terms of dual variable $\alpha$, the same term can be obtained as in eq. \ref{eq:SVM-rewritten}:
\begin{equation} 
L(\alpha) = \sum_i \alpha_i - \frac{1}{2} \sum_j \sum_i \alpha_j \alpha_i y_j y_i \mathbf{x}_i^T \mathbf{x}_j
\end{equation}
, and the new optimization problem states:
\begin{equation} 
\max L(\alpha) \,\,\,\,\, s.t. \,\,\, \left\{\begin{array}{lr} \alpha_i \geq 0 \\
\lambda_i \geq 0  \end{array}\right.
\end{equation}
However, since objective function $L(\alpha)$ does not depend on dual variable $\lambda_i$, the substituation can be made following the expression in eq. \ref{dual_variable_beta} and the new optimization problem states:
 \begin{equation} 
\max L(\alpha) \,\,\,\,\, s.t. \,\,\, 0 \leq \alpha_i \leq \gamma.
\end{equation}

This is the only diference between hard margin SVM and soft margin SVM.

It is important to state that the optimization problem does not depend on $\mathbf{x}$, but on $\mathbf{x}^T\mathbf{x}$. This allows the use of \emph{kernel trick} and implicitly enables nonlinear transform of the feature space at little additional cost. Usually, non-linear decision boundary can be achieved by nonlinear transform of features:
\begin{equation} 
\mathbf{x} \rightarrow \Phi(\mathbf{x})
\end{equation}

However, this operation is computationally expensive. The solution can be achieved using kernel functions. Kernel is a function $K(x,y)$ for which:
\begin{equation} 
K(\mathbf{x},\mathbf{y}) = \Phi(\mathbf{x})^T  \Phi(\mathbf{y})
\end{equation}

Since in the equation \ref{eq:SVM-dual} $\mathbf{x}$ does not appear by itself, but in a form of dot product $\mathbf{x}^T\mathbf{x}$, non-linear transform can be used in a form of kernel trick:
\begin{equation} 
L(\alpha) = \sum_i \alpha_i - \frac{1}{2} \sum_j \sum_i \alpha_j \alpha_i y_j y_i K(\mathbf{x}_i, \mathbf{x}_j)
\end{equation}

Most often used kernel is a radial basis kernel ($K_{RBF}(\mathbf{x}_i,\mathbf{x}_j)$):
\begin{equation} 
K_{RBF}(\mathbf{x}_i,\mathbf{x}_j) = e^\frac{-\parallel \mathbf{x}_i - \mathbf{x}_j\parallel^2}{2\sigma^2}
\end{equation}

Although SVM is conceptually designed as a two-class classifier, techniques for multiclass classification also exist, e.g. \emph{one-versus-one} or \emph{one-versus-all}.

\end{appendix}