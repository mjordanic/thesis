\documentclass{article}

\usepackage[margin=2.5cm]{geometry}

\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{threeparttable}
\usepackage{nicefrac}
\usepackage{cancel}

\newcommand{\argmax}{\arg\!\max} % AlfC

%\def\myquote#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\def\myquote{\list{}{\setlength{\leftmargin}{2cm}\setlength{\rightmargin}{0.5cm}}\item[]}
\let\endmyquote=\endlist 


\begin{document}
%commetn sve do ovdje



\section{Pattern Recognition}

Pattern recognition is a technique whose goal is classification and the principle by which it is performed is learned independetly from the data, i.e., training set. There are two main types of pattern recognition: supervised and unsupervised. Supervised pattern recognition implies that the classes of the training set are known and are used to obtain the model. New inputs are identified as one of the predetermined classes. On the other hand, unsuperised pattern recognition is used when no labels are available and samples are asigned to unknown classes. This technique is more appropriate for the clustering problem because the classes are determined automatically by the system, whereas supervised approach is more appropriate for the classification because classes are defined by the system designer.

In statistical pattern recognition, each sample is composed of m features $(x_0, x_1, \dots, x_{m-1})$. The goal of the algorithm is to obtain a decision rule, i.e., the decision boundary which separates well samples of different classes. There are many state of the art classifiers that use various principles to construct these boundaries. However, many researchers agree that the quality of pattern recognition depends mostly on selection of features. In other words, with appropriate selection of features, all classifiers will give similar classification result. Since pattern recognition is not the topic of this thesis, only a short introduction is provided on the two methods used in the thesis: Linear Discriminant Analysis and Support Vector Machine.


\subsection{Linear Discriminant Analysis}

\begin{myquote}
\begin{flushright}
\textit{All models are wrong; some models are useful.} \\George E. P. Box
\end{flushright}
\end{myquote}

Linear Discriminant Analysis is a computationally simple and efficient classifier with linear decision boundary, based on the Bayesian equation.

In a classical problem with $n$ samples in training set $\mathbf{x}$, which consist of $m$ features, the dataset of available samples is a matrix $\mathbf{x}_{[n \times m]}$, whereas the vector of labels that describe the belonging of each sample to one of the classes is $\mathbf{y}$, where $y \in (1,2,3,...,K)$.

According to Bayesian equation, the probability that a sample $\mathbf{x}_0$ belongs to a class $k$ is equivalent to the: 
\begin{equation} 
P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0) = \frac{P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=k) \,\,P(y=k)} {P(\mathbf{x}=\mathbf{x}_0)}
\end{equation}

Term $P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=k)$ is called the \emph{class-conditional probability} and describes the probability that the sample with exact features $\mathbf{x}_0$ is encountered within the group of samples belonging only to the class $k$. Term $P(y=k)$ is called the \emph{a priori probability} and describes the probability that the sample with class $k$ is found within the group of all samples, regardless of the features. Finally, the term $P(\mathbf{x}=\mathbf{x}_0)$ is called \emph{marginal probability} and describes the probability of finding the exact set of features in the dataset, regardless of the class. Marginal probability can be writen as a sum of class-conditional probabilities multiplied by the a priori probabilities for each class:

\begin{equation}
\begin{split}
P(\mathbf{x}=\mathbf{x}_0) = P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=1) \,\,P(y=1) + \\
P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=2) \,\,P(y=2) + \dots + \\
P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=K) \,\,P(y=K)
\end{split}
\end{equation}

For classification task, the hypothesis, i.e., the predicted class of a sample $\mathbf{x}_0$ is chosen as the class which has the highest probability $P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0)$:

\begin{equation} 
h(\mathbf{x}_0) = \argmax_k P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0)
\end{equation}

Statistically speaking, this is the best possible classifier. The problem arises in implementation. The exact probability density functions are unknown and have to be estimate from the available data, which is the source of error.
Estimated version of the stated probability will be marked with a different symbols to stress out the fact they are just an estimates:

\begin{equation} 
p_k(\mathbf{x}) \coloneqq P(y=k \,\, | \, \mathbf{x})
\end{equation}

\begin{equation} 
g_k(\mathbf{x}) \coloneqq P(\mathbf{x}  \,\, | \, y=k)
\end{equation}

\begin{equation} 
\pi_k \coloneqq P(y=k)
\end{equation}


Linear Discriminant Analysis estimates class-conditial probability term in the Bayesian equation from the available dataset as a Gaussian function:

\begin{equation} 
g_k(\mathbf{x}) = \frac{1}{(2 \pi)^{\nicefrac{1}{2}}\, |\Sigma_k|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_k)^T  \Sigma_k^{-1} (\mathbf{x}-\mu_k)} 
\end{equation}

, where $\mu_k$ and $\Sigma_k$ are the mean and covariance matrix for class $k$, respectively, and they are estimated from the available data as:

\begin{equation}
\left. m_k = \frac{1}{n_k} \sum_{i}{\mathbf{x}_i} \right\vert_{\forall \mathbf{x} \in k}
\end{equation}

\begin{equation}
\left. \Sigma_k = \frac{1}{n_k-K} \sum_{i}{\Big( \mathbf{x}_i - \mu_k \Big) \Big( \mathbf{x}_i - \mu_k \Big)^T} \right\vert_{\forall \mathbf{x} \in k}
\end{equation}


%However, Naïve Bayesian \emph{naively} assumes statistical independence between features, i.e., off diagonal elements of the covariance matrix $\Sigma$ are zero.
 
To simplify the model, LDA assumes that the covariance matrices $\Sigma$ are the same for all classes:

\begin{equation} 
\Sigma_0 = \Sigma_1 = \dots = \Sigma_K = \Sigma
\end{equation}

and they are usually calculated using weighted average:
\begin{equation} 
\Sigma = \frac{\sum_{k=1}^K {n_k\Sigma_k}}{\sum_{k=1}^K{n_k}}
\end{equation}

where $n_k$ represents the number of samples belonging to a class $k$. This assumption also 

In a two class example ($y \in \{0,1\}$), all samples on the decision boundary ($D.B.$) will have the same probability of belonging to class 0 or 1:

\begin{equation} 
D.B. = \Big\{\mathbf{x}\,\, \Big| \,\,P\big(y=0 \, \big| \, \mathbf{x}=\mathbf{x}_0\big) = P\big(y=1 \, \big| \, \mathbf{x}=\mathbf{x}_0\big) \Big\}
\end{equation}

Following this idea, the decision boundary can be estimated by solving the equation:

\begin{equation} 
\frac{g_0(\mathbf{x}) \, \pi_0} {\cancel{\sum_{k=1}^K{g_k \, \pi_k}}} = \frac{g_1(\mathbf{x}) \, \pi_1} {\cancel{\sum_{k=1}^K{g_k \, \pi_k}}}
\end{equation}


\begin{equation} 
\begin{split}
\frac{1}{(2 \pi)^{\nicefrac{d}{2}}\, |\Sigma_0|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_0)^T  \Sigma_0^{-1} (\mathbf{x}-\mu_0)}  \,\,\,\pi_0 = \\
\frac{1}{(2 \pi)^{\nicefrac{d}{2}}\, |\Sigma_1|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_1)^T  \Sigma_1^{-1} (\mathbf{x}-\mu_1)}  \,\,\,\pi_1
\end{split}
\end{equation}

If making the assumption on the equal covariance matrices for both classes:
\begin{equation} 
\Sigma_0 = \Sigma_1  = \Sigma
\end{equation}

and taking the logarithm, the equation takes the form: 

\begin{equation} 
\begin{split}
-\frac{1}{2}  \Big(\mathbf{x}-\mu_0\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_0\Big) +  \log{\Big(\pi_0\Big)}= \\
-\frac{1}{2}  \Big(\mathbf{x}-\mu_1\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_1\Big) +  \log{\Big(\pi_1)\Big)}
\end{split}
\end{equation}

This equation can be written as the linear function $x^T\beta + \alpha = 0$ as:

\begin{equation} 
\begin{split}
\mathbf{x}^T\Big(\Sigma^{-1} \mu_0 - \Sigma^{-1} \mu_1\Big) + \frac{1}{2} \Big(\mu_1^T \Sigma^{-1}\mu_1 - \mu_0^T \Sigma^{-1}\mu_0\Big)
+ \log\Big( \frac{\pi_0}{\pi_1}  \Big) = 0
\end{split}
\end{equation}

The equation represents the decision boundary between classes, i.e., all samples lying on this line will have equal probability of belonging to class 0 and class 1. It is interesting to note that the slope of the line depends only on the class means and covariance matrix, whereas a priori probabilities (which are the result of number of samples belonging to class 0 or 1) have effect only on the $y$-intercept term, i.e., the offset of the function.

When considering multiclass classification problem, probability of a sample belonging to each class is firstly estimated by the equation:

\begin{equation}
p_k = -\frac{1}{2} \log \big\vert \Sigma \big\vert - \frac{1}{2}  \Big(\mathbf{x}-\mu_k\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_k\Big) +  \log{\Big(\pi_k\Big)}
\end{equation}

and then the class is estimated as the one with the highest probability as:

\begin{equation} 
h(\mathbf{x}) = \argmax_k p_k(\mathbf{x})
\end{equation}


\subsection{Support Vector Machine}

\begin{myquote}
\begin{flushright}
\textit{Try to solve the problem directly and never solve a more general problem as an intermediate step.} \\Vladimir Vapnik
\end{flushright}
\end{myquote}


Supoort vector machine is nowadays known as a very powerful classifier with a lot of different applications. The big advantage over LDA is the fact that it is a \emph{non-paramteric} classifier. The model is not obtained using assumption of the form of the class density function and estimation of it's parameters, which is inevitably erraneous. Instead, SVM forms the decision boundary using the samples itselves (not their density estiamtes) by maximizing the distance between samples and the boundary.
This was the idea Vladimir Vapnik, the inventor of this method stood for. It is better to try to solve the problem directly and simply, without many intermediate steps that can be complicated and inacurate.

This classification method esentially has a very simple principle, but using several mathemathical trics, it became a very powerful tool. The optimization problem does not depend on $\mathbf{x}$, but on $\mathbf{x}^T\mathbf{x}$. This enables the use of \emph{kernel trick} and allows nonlinear transform of the feature space at no additional cost. 













Term $P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=k)$ is called the \emph{class-conditional probability} and describes the probability that the sample with exact features $\mathbf{x}_0$ is encountered within the group of samples belonging only to the class $k$. Term $P(y=k)$ is called the \emph{a priori probability} and describes the probability that the sample with class $k$ is found within the group of all samples, regardless of the features. Finally, the term $P(\mathbf{x}=\mathbf{x}_0)$ is called \emph{marginal probability} and describes the probability of finding the exact set of features in the dataset, regardless of the class. Marginal probability can be writen as a sum of class-conditional probabilities multiplied by the a priori probabilities for each class:

\begin{equation}
\begin{split}
P(\mathbf{x}=\mathbf{x}_0) = P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=1) \,\,P(y=1) + \\
P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=2) \,\,P(y=2) + \dots + \\
P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=K) \,\,P(y=K)
\end{split}
\end{equation}

For classification task, the hypothesis, i.e., the predicted class of a sample $\mathbf{x}_0$ is chosen as the class which has the highest probability $P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0)$:

\begin{equation} 
h(\mathbf{x}_0) = \argmax_k P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0)
\end{equation}

Statistically speaking, this is the best possible classifier. The problem arises in implementation. The exact probability density functions are unknown and have to be estimate from the available data.
Estimated version of the stated probability will be marked with a different symbols to stress out the fact they are just an estimates:

\begin{equation} 
p_k(\mathbf{x}) \coloneqq P(y=k \,\, | \, \mathbf{x})
\end{equation}

\begin{equation} 
g_k(\mathbf{x}) \coloneqq P(\mathbf{x}  \,\, | \, y=k)
\end{equation}

\begin{equation} 
\pi_k \coloneqq P(y=k)
\end{equation}


\subsection{Linear Discriminant Analysis}

\begin{flushright}
\textit{All models are wrong; some models are useful.} \\George E. P. Box
\end{flushright}

Linear Discriminant Analysis is a computationally simple and efficient classifier with linear decision boundary.
It is based on the Bayesian equation and estimates class-conditial probability from the available dataset as a Gaussian function:

\begin{equation} 
g_k(\mathbf{x}) = \frac{1}{(2 \pi)^{\nicefrac{1}{2}}\, |\Sigma_k|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_k)^T  \Sigma_k^{-1} (\mathbf{x}-\mu_k)} 
\end{equation}

, where $\mu_k$ and $\Sigma_k$ are the mean and covariance matrix for class $k$, respectively, and they are estimated from the available data as:

\begin{equation}
\left. m_k = \frac{1}{n_k} \sum_{i}{\mathbf{x}_i} \right\vert_{\forall \mathbf{x} \in k}
\end{equation}

\begin{equation}
\left. \Sigma_k = \frac{1}{n_k-K} \sum_{i}{\Big( \mathbf{x}_i - \mu_k \Big) \Big( \mathbf{x}_i - \mu_k \Big)^T} \right\vert_{\forall \mathbf{x} \in k}
\end{equation}


%However, Naïve Bayesian \emph{naively} assumes statistical independence between features, i.e., off diagonal elements of the covariance matrix $\Sigma$ are zero.
 
To simplify the model, LDA assumes that the covariance matrices $\Sigma$ are the same for all classes:

\begin{equation} 
\Sigma_0 = \Sigma_1 = \dots = \Sigma_K = \Sigma
\end{equation}

and they are usually calculated using weighted average:
\begin{equation} 
\Sigma = \frac{\sum_{k=1}^K {n_k\Sigma_k}}{\sum_{k=1}^K{n_k}}
\end{equation}

where $n_k$ represents the number of samples belonging to a class $k$. This assumption also 

In a two class example ($y \in \{0,1\}$), all samples on the decision boundary ($D.B.$) will have the same probability of belonging to class 0 or 1:

\begin{equation} 
D.B. = \Big\{\mathbf{x}\,\, \Big| \,\,P\big(y=0 \, \big| \, \mathbf{x}=\mathbf{x}_0\big) = P\big(y=1 \, \big| \, \mathbf{x}=\mathbf{x}_0\big) \Big\}
\end{equation}

Following this idea, the decision boundary can be estimated by solving the equation:

\begin{equation} 
\frac{g_0(\mathbf{x}) \, \pi_0} {\cancel{\sum_{k=1}^K{g_k \, \pi_k}}} = \frac{g_1(\mathbf{x}) \, \pi_1} {\cancel{\sum_{k=1}^K{g_k \, \pi_k}}}
\end{equation}


\begin{equation} 
\begin{split}
\frac{1}{(2 \pi)^{\nicefrac{d}{2}}\, |\Sigma_0|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_0)^T  \Sigma_0^{-1} (\mathbf{x}-\mu_0)}  \,\,\,\pi_0 = \\
\frac{1}{(2 \pi)^{\nicefrac{d}{2}}\, |\Sigma_1|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_1)^T  \Sigma_1^{-1} (\mathbf{x}-\mu_1)}  \,\,\,\pi_1
\end{split}
\end{equation}

If making the assumption on the equal covariance matrices for both classes:
\begin{equation} 
\Sigma_0 = \Sigma_1  = \Sigma
\end{equation}

and taking the logarithm, the equation takes the form: 

\begin{equation} 
\begin{split}
-\frac{1}{2}  \Big(\mathbf{x}-\mu_0\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_0\Big) +  \log{\Big(\pi_0\Big)}= \\
-\frac{1}{2}  \Big(\mathbf{x}-\mu_1\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_1\Big) +  \log{\Big(\pi_1)\Big)}
\end{split}
\end{equation}

This equation can be written as the linear function $x^T\beta + \alpha = 0$ as:

\begin{equation} 
\begin{split}
\mathbf{x}^T\Big(\Sigma^{-1} \mu_0 - \Sigma^{-1} \mu_1\Big) + \frac{1}{2} \Big(\mu_1^T \Sigma^{-1}\mu_1 - \mu_0^T \Sigma^{-1}\mu_0\Big)
+ \log\Big( \frac{\pi_0}{\pi_1}  \Big)
\end{split}
\end{equation}

The equation represents the decision boundary between classes, i.e., all samples lying on this line will have equal probability of belonging to class 0 and class 1. It is interesting to note that the slope of the line depends only on the class means and covariance matrix, whereas a priori probabilities (which are the result of number of samples belonging to class 0 or 1) have effect only on the $y$-intercept term, i.e., the offset of the function.

When considering multiclass classification problem, probability of a sample belonging to each class is firstly estimated by the equation:

\begin{equation}
p_k = -\frac{1}{2} \log \big\vert \Sigma \big\vert - \frac{1}{2}  \Big(\mathbf{x}-\mu_k\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_k\Big) +  \log{\Big(\pi_k\Big)}
\end{equation}

and then the class is estimated as the one with the highest probability as:

\begin{equation} 
h(\mathbf{x}) = \argmax_k p_k(\mathbf{x})
\end{equation}


\subsection{Support Vector Machine}

\begin{myquote}
\begin{flushright}
\textit{Try to solve the problem directly and never solve a more general problem as an intermediate step.} \\Vladimir Vapnik
\end{flushright}
\end{myquote}




Supoort vector machine is nowadays known as a very powerful classifier with a lot of different applications. The big advantage over LDA is the fact that it is a \emph{non-paramteric} classifier. This implies that the model is not obtained using assumption of the form of the class density function and estimation of it's parameters, which is inevitably erraneous. Instead, SVM forms the decision boundary between samples by maximizing the distance between samples and the boundary.
As Vladimir Vapnik said ''When solving a problem of interest, do not solve a more general problem as an intermediate step.''

This classification method esentially has a very simple principle, but using sever mathemathical trics, it became a very powerful tool. The optimization problem does not depend on $\mathbf{x}$, but on $\mathbf{x}^T\mathbf{x}$. This enables the use of \emph{kernel trick} and allows nonlinear transform of the feature space at no additional cost. 




\end{document}