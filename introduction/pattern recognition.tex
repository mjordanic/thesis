\documentclass{article}

\usepackage[margin=2.5cm]{geometry}

\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{threeparttable}
\usepackage{nicefrac}
\usepackage{cancel}

\newcommand{\argmax}{\arg\!\max} % AlfC

%\def\myquote#1#2{\list{}{\rightmargin#2\leftmargin#1}\item[]}
\def\myquote{\list{}{\setlength{\leftmargin}{2cm}\setlength{\rightmargin}{0.5cm}}\item[]}
\let\endmyquote=\endlist 


\begin{document}
%commetn sve do ovdje



\section{Pattern Recognition}

Pattern recognition is a technique whose goal is classification and the principle by which it is performed is learned independently from the data, i.e., training set. There are two main types of pattern recognition: supervised and unsupervised. Supervised pattern recognition implies that the classes of the training set are known and are used to obtain the model. New inputs are identified as one of the predetermined classes. On the other hand, unsupervised pattern recognition is used when no labels are available and samples are assigned to unknown classes. This technique is more appropriate for the clustering problem because the classes are determined automatically by the system, whereas supervised approach is more appropriate for the classification because classes are defined by the system designer.

In statistical pattern recognition, each sample is composed of $m$ measures that form the pattern, i.e., features $(x_0, x_1, \dots, x_{m-1})$. The goal of the algorithm is to obtain a decision rule, i.e., the decision boundary which separates well samples of different classes. There are many \emph{state-of-the-art} classifiers that use various principles to construct these boundaries. However, many researchers agree that the fidelity of the classification in EMG applications depends mostly on selection of features. In other words, with appropriate selection of features, all classifiers will give similar classification result. Since pattern recognition is not the topic of this thesis, only a short introduction is provided on the two methods used in the thesis: Linear Discriminant Analysis and Support Vector Machine.


\subsection{Linear Discriminant Analysis}

\begin{myquote}
\begin{flushright}
\textit{All models are wrong; some models are useful.} \\George E. P. Box
\end{flushright}
\end{myquote}

Linear Discriminant Analysis is a computationally simple and efficient classifier with linear decision boundary and it is based on the Bayesian equation.

In a classical problem with $n$ samples in training set, which consist of $m$ features, the dataset of available samples is a matrix $\mathbf{x}_{[n \times m]}$, whereas the vector of labels that describe the belonging of each sample to one of the classes is $\mathbf{y}$, where $y \in (0,1,2,...,K-1)$.

According to Bayesian equation, the probability that a sample $\mathbf{x}_0$ belongs to a class $k$ is equivalent to the: 
\begin{equation} 
P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0) = \frac{P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=k) \,\,P(y=k)} {P(\mathbf{x}=\mathbf{x}_0)}
\end{equation}

,where k represents the class. Term $P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=k)$ is called the \emph{class-conditional} probability and describes the probability that the sample with exact features $\mathbf{x}_0$ is encountered within the group of samples belonging only to the class $k$. Term $P(y=k)$ is called the \emph{a priori} probability and describes the probability that the sample with class $k$ is found within the group of all samples, regardless of the features. Finally, the term $P(\mathbf{x}=\mathbf{x}_0)$ is called \emph{marginal} probability and describes the probability of finding the exact set of features in the dataset, regardless of the class. Marginal probability can be written as a sum of class-conditional probabilities multiplied by the a priori probabilities for each class:

\begin{equation}
\begin{split}
P(\mathbf{x}=\mathbf{x}_0) = P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=1) \,\,P(y=1) + \\
P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=2) \,\,P(y=2) + \dots + \\
P(\mathbf{x}=\mathbf{x}_0  \,\, | \, y=K) \,\,P(y=K)
\end{split}
\end{equation}

Following Bayesian theory, the hypothesis, i.e., the predicted class of a sample $\mathbf{x}_0$ is chosen as the class which has the highest probability $P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0)$:

\begin{equation} 
h(\mathbf{x}_0) = \argmax_k P(y=k \,\, | \, \mathbf{x}=\mathbf{x}_0)
\end{equation}

Statistically speaking, this is the best possible classifier. The problem arises in implementation. The exact probability density functions are unknown and have to be estimate from the available data, which is the source of error.
Estimated version of the stated probabilities will be marked with a different symbols to stress out the fact they are just an estimates:

\begin{equation} 
p_k(\mathbf{x}) \coloneqq P(y=k \,\, | \, \mathbf{x})
\end{equation}

\begin{equation} 
g_k(\mathbf{x}) \coloneqq P(\mathbf{x}  \,\, | \, y=k)
\end{equation}

\begin{equation} 
\pi_k \coloneqq P(y=k)
\end{equation}


Linear Discriminant Analysis estimates marginal probability term ($\pi_k$) as ratio of number of samples belonging to class $k$ and the total  number of samples, whereas class-conditional probability term in the Bayesian equation is estimated as a Gaussian function:

\begin{equation} 
g_k(\mathbf{x}) = \frac{1}{(2 \pi)^{\nicefrac{1}{2}}\, |\Sigma_k|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_k)^T  \Sigma_k^{-1} (\mathbf{x}-\mu_k)} 
\end{equation}

, where $g_k$ is estimated class-conditional probability of class $k$, and $\mu_k$ and $\Sigma_k$ are the mean and co-variance matrix for class $k$, respectively, and they are estimated from the available data as:

\begin{equation}
\left. \mu_k = \frac{1}{n_k} \sum_{i}{\mathbf{x}_i} \right\vert_{\forall \mathbf{x} \in k}
\end{equation}

\begin{equation}
\left. \Sigma_k = \frac{1}{n_k-K} \sum_{i}{\Big( \mathbf{x}_i - \mu_k \Big) \Big( \mathbf{x}_i - \mu_k \Big)^T} \right\vert_{\forall \mathbf{x} \in k}
\end{equation}

 
where $n_k$ represents the number of samples belonging to a class $k$.
To simplify the model, LDA assumes that the co-variance matrices $\Sigma_k$ are the same for all classes:

\begin{equation} 
\Sigma_0 = \Sigma_1 = \dots = \Sigma_{K-1} = \Sigma
\end{equation}

and they are usually calculated using weighted average:
\begin{equation} 
\Sigma = \frac{\sum_{k=1}^K {n_k\Sigma_k}}{\sum_{k=1}^K{n_k}}
\end{equation}

The consequence of this assumption is the linearity of the decision boundary. % The lack of this simplification would lead to quadratic discriminant analysis, which has non-linear boundary.

In a two class example ($y \in \{0,1\}$), all samples on the decision boundary ($D.B.$) will have the same probability of belonging to class 0 or 1:

\begin{equation} 
D.B. = \Big\{\mathbf{x}\,\, \Big| \,\,P\big(y=0 \, \big| \, \mathbf{x}=\mathbf{x}_0\big) = P\big(y=1 \, \big| \, \mathbf{x}=\mathbf{x}_0\big) \Big\}
\end{equation}

Following this idea, the decision boundary can be estimated by solving the equation:

\begin{equation} 
\frac{g_0(\mathbf{x}) \, \pi_0} {\cancel{\sum_{k=1}^K{g_k \, \pi_k}}} = \frac{g_1(\mathbf{x}) \, \pi_1} {\cancel{\sum_{k=1}^K{g_k \, \pi_k}}}
\end{equation}


\begin{equation} 
\begin{split}
\frac{1}{(2 \pi)^{\nicefrac{d}{2}}\, |\Sigma_0|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_0)^T  \Sigma_0^{-1} (\mathbf{x}-\mu_0)}  \,\,\,\pi_0 = \\
\frac{1}{(2 \pi)^{\nicefrac{d}{2}}\, |\Sigma_1|^{\nicefrac{1}{2}}}\,\, e^{\nicefrac{-1}{2}  (\mathbf{x}-\mu_1)^T  \Sigma_1^{-1} (\mathbf{x}-\mu_1)}  \,\,\,\pi_1
\end{split}
\end{equation}

,where $d$ is the dimensionality of the feature space, i.e., number of features representing each sample.
If making the assumption on the equal co-variance matrices for both classes:
\begin{equation} 
\Sigma_0 = \Sigma_1  = \Sigma
\end{equation}

and taking the logarithm, the equation takes the form: 

\begin{equation} 
\begin{split}
-\frac{1}{2}  \Big(\mathbf{x}-\mu_0\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_0\Big) +  \log{\Big(\pi_0\Big)}= \\
-\frac{1}{2}  \Big(\mathbf{x}-\mu_1\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_1\Big) +  \log{\Big(\pi_1)\Big)}
\end{split}
\end{equation}

This equation can be written as the linear function $x^T\beta + \alpha = 0$ as:

\begin{equation} 
\begin{split}
\mathbf{x}^T\Big(\Sigma^{-1} \mu_0 - \Sigma^{-1} \mu_1\Big) + \frac{1}{2} \Big(\mu_1^T \Sigma^{-1}\mu_1 - \mu_0^T \Sigma^{-1}\mu_0\Big)
+ \log\Big( \frac{\pi_0}{\pi_1}  \Big) = 0
\end{split}
\end{equation}

The equation represents the decision boundary between classes, i.e., all samples lying on this line will have equal probability of belonging to class 0 and class 1. It is interesting to note that the slope of the line depends only on the class means and co-variance matrix, whereas a priori probabilities (which are the result of number of samples belonging to class 0 or 1) have effect only on the $y$-intercept term, i.e., the offset of the function. This is an interesting point that demands caution. If groups are unbalanced, that is, number of samples of one group is higher than in the other group, $y$-intercept of the decision boundary will be affected and the classifier will be biased by this disproportion.

When considering multiclass classification problem, probability of a sample belonging to each class is firstly estimated by the equation:

\begin{equation}
p_k = -\frac{1}{2} \log \big\vert \Sigma \big\vert - \frac{1}{2}  \Big(\mathbf{x}-\mu_k\Big)^T  \Sigma^{-1} \Big(\mathbf{x}-\mu_k\Big) +  \log{\Big(\pi_k\Big)}
\end{equation}

and then the class is estimated as the one with the highest probability as:

\begin{equation} 
h(\mathbf{x}) = \argmax_k p_k(\mathbf{x})
\end{equation}


\subsection{Support Vector Machine}

\begin{myquote}
\begin{flushright}
\textit{Try to solve the problem directly and never solve a more general problem as an intermediate step.} \\Vladimir Vapnik
\end{flushright}
\end{myquote}


Support vector machine is nowadays known as a very powerful classifier with a lot of different applications. The big advantage over LDA is the fact that it is a \emph{non-parametric} classifier. The model is not obtained using assumption of the form of the class density function and estimation of it's parameters, which is inevitably erroneous. Instead, SVM forms the decision boundary using the samples (not their density estimates) by maximizing the distance between samples and the boundary.
This was the idea Vladimir Vapnik, the inventor of this method stood for. It is better to try to solve the problem directly and simply, without many intermediate steps that can be complicated and inaccurate.

This classification method essentially has a very simple principle, but using several mathematical tricks, it became a very powerful tool. The optimization problem does not depend on $\mathbf{x}$, but on $\mathbf{x}^T\mathbf{x}$. This enables the use of \emph{kernel trick} and allows nonlinear transform of the feature space at no additional cost. 












\end{document}